# ============================================
# llm_manager.py
# Unified, environment-aware LLM controller
# (Auto-selects OpenAI ‚Üí Hugging Face ‚Üí Ollama ‚Üí Fallback)
# ============================================

from app.llms.ollama import call_ollama
from app.llms.huggingface import call_huggingface
from app.llms.gemini import call_gemini
from app.llms.openai import call_openai
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from dotenv import load_dotenv
import logging
import os

# ------------------------------------------------
# Load configuration
# ------------------------------------------------
load_dotenv()
logger = logging.getLogger("llm-manager")

LLM_MODE = os.getenv("LLM_MODE", "openai").lower()  # auto | huggingface | ollama | openai | gemini
FALLBACK_MODEL = "distilgpt2"
# ------------------------------------------------
# Device detection
# ------------------------------------------------
def detect_device():
    """Detect best available compute device."""
    if torch.cuda.is_available():
        return "cuda"
    elif torch.backends.mps.is_available():
        return "mps"
    return "cpu"

DEVICE = detect_device()
logger.info(f"üß† Using device: {DEVICE.upper()}")

# ------------------------------------------------
# Core LLM handler
# ------------------------------------------------
def call_llm(prompt: str, LLM_MODE: str = LLM_MODE) -> str:
    """
    Unified function to route query based on availability:
    1Ô∏è‚É£ OpenAI (if API key exists)
    2Ô∏è‚É£ Hugging Face Transformers
    3Ô∏è‚É£ Ollama (local)
    4Ô∏è‚É£ Offline Fallback
    """

    # =======================
    # 1Ô∏è‚É£ OpenAI
    # =======================
    if LLM_MODE in ["auto", "openai"]:
        return call_openai(prompt)

    # =======================
    # 1Ô∏è‚É£ Gemini
    # =======================
    if LLM_MODE in ["gemini"]:
       return call_gemini(prompt)
    
    # =======================
    # 2Ô∏è‚É£ Hugging Face
    # =======================
    if LLM_MODE in ["huggingface"]:
        return call_huggingface(prompt)

    # =======================
    # 3Ô∏è‚É£ Ollama
    # =======================
    if LLM_MODE in ["ollama"]:
        return call_ollama(prompt)

    # =======================
    # 4Ô∏è‚É£ Offline fallback
    # =======================
    try:
        logger.info("‚öôÔ∏è Using offline fallback model.")
        tokenizer = AutoTokenizer.from_pretrained(FALLBACK_MODEL)
        model = AutoModelForCausalLM.from_pretrained(FALLBACK_MODEL)
        inputs = tokenizer(prompt, return_tensors="pt")
        outputs = model.generate(**inputs, max_new_tokens=128)
        return tokenizer.decode(outputs[0], skip_special_tokens=True)
    except Exception as e:
        logger.error(f"‚ùå All backends failed: {e}")
        return "I'm currently unable to answer due to system issues."



